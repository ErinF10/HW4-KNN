# -*- coding: utf-8 -*-
"""Hw2_answer_erin_forrest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vunHswdT0OppCPtotGrTvPE85jFl37eY

---

```
CISC 3440: Machine Learning
Hw2 (4 Points)
Due: Oct 21st, 11:59 pm
```
---

We will use the Titanic dataset (P1) for this. You are responsible for writing the code for KNN classifier. We follow the same pre-processing steps we used in the Decision Tree Tutorial.

**Pre-processing and Dataset creation**

We will start by reading the data and displaying it. Each line in the file corresponds to sample with five values. The first four values are the features and the last one is the label. You can read the iris.names file to know more about the different features and attributes.

The next step is to create training, validation, and test sets. The training, validation, and test splits are 80%, 5%, and 15% respectively. The code below use random to randomly assign the sample indices to one of the three sets
"""

def get_titanic_data():
  """
  Load the training and the validation files. When the testFiles are not present will return None for them.

  Parameters:
  -----------
  None

  Returns:
  -----------

  train_features: numpy array

  train_labels: numpy array

  validation_features: numpy array

  validation_labels: numpy array

  test_features: numpy array

  test_labels: numpy array
  """
  import numpy as np
  train_features = np.load('train_features_array.npy')
  train_labels = np.load('train_labels_array.npy')
  validation_features = np.load('validation_features_array.npy')
  validation_labels = np.load('validation_labels_array.npy')
  try:
    test_features = np.load('test_features_array.npy')
    test_labels = np.load('test_labels_array.npy')
  except:
    print("**Looks like we are not testing yet setting the test data to None**\n")
    test_features=None
    test_labels=None
  return train_features,train_labels,validation_features,validation_labels,test_features,test_labels

get_titanic_data()
import numpy as np
train_features = np.load('train_features_array.npy')
train_labels = np.load('train_labels_array.npy')
validation_features = np.load('validation_features_array.npy')
validation_labels = np.load('validation_labels_array.npy')
print(len(train_features))
print(len(validation_features))

"""**Euclidean Distance**

For the KNN algorithm, we will use the euclidean distance as the distance metric. Please use the function implementation below in your code.
"""

def euclidean_distance(x,y):
    """
    Calculate the Euclidean distance between two vectors.

    This function computes the Euclidean distance between two equal-length
    vectors `x` and `y`.

    Parameters:
    -----------
    x : numpy.ndarray


    y : numpy.ndarray


    Returns:
    --------
    distance: float

    Example:
    --------
    >>> import numpy as np
    >>> x = np.array([1, 2, 3])
    >>> y = np.array([4, 5, 6])
    >>> euclidean_distance(x, y)
    3.0
    """
    import math
    sum=0
    for i in range(x.shape[0]):
      sum+=(x[i]-y[i])**2
    return math.sqrt(sum)

"""**KNN Classifier**

Below is the function definition that should implement the KNN classifier. Implement the algorithm in the provided space. The function should take one test sample along with the training features and labels and return the label for the test sample. You will use the implementation of *euclidean_distance()* above.
"""

def knn_classifier(training_data,training_labels, test_sample,K):
  """

    Parameters:
    -----------
    training_data : list of lists or 2D array

    training_labels : list

    test_sample : list or array

    K : int


    Returns:
    --------
    label :
        The predicted label for the test sample.

    Notes:
    ------
    - The function uses Euclidean distance, use the above implementation.

  """

  #Step 1: Compute euclidian distances
  euclidean_distances = np.zeros((len(training_data), 2))

  for i, feature in enumerate(training_data):
    distance = 0
    distance = euclidean_distance(feature, test_sample)
    euclidean_distances[i, 0] = distance
    euclidean_distances[i, 1] = i #keep track of the origional indexes before sorting

  print("Distances before sorting: ")
  print(euclidean_distances, end="\n\n")

  #Step 2: Sort distances

  euclidean_distances = euclidean_distances[euclidean_distances[:, 0].argsort()]

  print("Distances after sorting: ")
  print(euclidean_distances, end="\n\n")


  #Step 3/4: Select K Points with the shortest distance and count the occurences

  zero_count = 0
  one_count = 0

  for i in range(K):
    j = int(euclidean_distances[i, 1])
    if training_labels[j] == 0:
      zero_count += 1
    else:
      one_count += 1

  print('zero count:', zero_count)
  print('one count:', one_count, end="\n\n")

  #Step 5: return class with highest count

  if zero_count > one_count:
    return 0
  return 1

label = knn_classifier(train_features,train_labels, validation_features[1], 5)
print('Label: ', label)

"""**Hyper-parameter validation**

The code below, generates the Accuracy plot for differet K-values, choose a K-value based on the generated plot from your implementation and assign it to the variable **K which is currently set to None**
"""

train_features,train_labels,validation_features,validation_labels,test_features,test_labels=get_titanic_data()
k_dict={}
needs_implementation = False
for i in range(10):
  correct_count=0
  for x,y in zip(validation_features,validation_labels):
    predicted_label=knn_classifier(train_features,train_labels,x,i+1)
    if predicted_label is None:
      needs_implementation=True
      break
    if predicted_label==y:
      correct_count+=1
  k_dict[i+1]=correct_count/len(validation_labels)

if needs_implementation:
  print("****Please complete the implementation of knn_classifier****")
else:
  import matplotlib.pyplot as plt
  print(k_dict)
  # Extracting K values and accuracy values
  K_values = list(k_dict.keys())
  accuracy_values = list(k_dict.values())

  plt.figure(figsize=(8, 6))
  plt.plot(K_values, accuracy_values, marker='o', linestyle='-', color='b', label='Accuracy')
  plt.xlabel('K (Number of Neighbors)')
  plt.ylabel('Accuracy')
  plt.title('K-NN Accuracy vs K')
  plt.grid(True)
  plt.legend()
  plt.show()

#Set the K-value here
K=None